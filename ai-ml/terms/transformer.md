# Transformer — 讓 AI 理解語言的革命性架構，ChatGPT、Claude 的底層技術

> **白話說：** 如果 AI 是一輛車，Transformer 就是那顆超強引擎。它讓 AI 能一次「看完整句話」，理解前後文關係，不用一個字一個字慢慢讀。

---

## 它到底是什麼？

Transformer 是 2017 年 Google 提出的一種 [Neural Network](neural-network.md) 架構。

在 Transformer 出現之前，AI 處理語言是「一個字一個字按順序讀」的，像一個人用手指一個字一個字指著讀——很慢，而且讀到後面常常忘了前面講什麼。

Transformer 最大的突破是一種叫做「注意力機制（Attention）」的技術：
- AI 能同時看到整句話的每一個字
- 而且能判斷「哪個字跟哪個字有關係」
- 就像你讀一句話時，眼睛是掃過整句的，不是一個字一個字看

這讓 AI 理解語言的能力突飛猛進。現在幾乎所有厲害的 AI——[GPT](gpt.md)、Claude、Gemini、LLaMA——底層都是 Transformer。

## 生活比喻 / 實際例子

想像你在看一部電影的劇本：

- **舊方法**：你只能一行一行看，看到第 100 頁時已經忘了第 1 頁在說什麼
- **Transformer**：你能同時攤開所有頁面，隨時對照任何兩個場景的關聯——「第 5 頁的伏筆原來在第 80 頁回收了！」

這種「同時看全局、找出關聯」的能力，就是 Transformer 的威力。

實際會聽到的說法：
- 「**Transformer** 架構是現代 AI 的基礎」
- 「那篇論文叫 Attention is All You Need，提出了 **Transformer**」
- 「所有主流 [LLM](llm.md) 都是基於 **Transformer** 的」

## 為什麼要知道這個詞？

Transformer 是過去幾年 AI 爆發式進步的最大功臣。如果你想理解為什麼 AI 突然變這麼厲害、為什麼 [GPT](gpt.md)、Claude 能跟人類自然對話，答案就是 Transformer。它是 [LLM](llm.md)、[GPT](gpt.md)、[NLP](nlp.md) 等技術的底層架構。

---
**[← 回到 AI / 機器學習總覽](../README.md)**
