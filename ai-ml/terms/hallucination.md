# Hallucination（幻覺） — AI 一本正經地胡說八道

> **白話說：** 就像一個很會掰的朋友——你問他什麼他都能回答，講得頭頭是道，但仔細查才發現他亂講的。AI 的幻覺就是「很有自信地說出錯誤的資訊」。

---

## 它到底是什麼？

Hallucination 是指 AI [模型](model.md)（尤其是 [LLM](llm.md)）生成了看起來合理、但實際上不正確的內容。

為什麼會這樣？因為 [LLM](llm.md) 的運作方式是「預測下一個最可能的字」，它不是真的「理解」內容。它只是根據 [訓練](training.md) 時學到的統計規律來生成文字。所以有時候它會：

- **捏造不存在的資訊**：引用一篇根本不存在的論文
- **張冠李戴**：把 A 公司的事蹟說成 B 公司的
- **編造細節**：說某個人在某年做了某件事，但那件事從來沒發生過

最可怕的是，AI 講這些錯誤內容時，語氣跟講正確的內容完全一樣——非常自信、非常流暢。你很難從語氣判斷它是對是錯。

## 生活比喻 / 實際例子

想像你問一個很愛面子的朋友：

- 你問：「台北 101 是哪一年蓋好的？」
- 他不確定，但他不想說「我不知道」
- 所以他很自信地說：「2002 年！」（其實是 2004 年完工）
- 你聽他這麼有自信，就信了

AI 的幻覺也是這樣——它「不知道自己不知道」，所以從來不會說「我不確定」。

實際會聽到的說法：
- 「ChatGPT 有時會 **Hallucinate**，回答的內容要自己查證」
- 「這個 AI 的 **幻覺** 問題很嚴重，不能直接拿來用」
- 「我們用 [RAG](rag.md) 來減少 **Hallucination**」

## 為什麼要知道這個詞？

如果你在用 ChatGPT、Claude 或任何 AI 工具，Hallucination 是你最需要警惕的事。AI 的回答不能照單全收，特別是涉及事實、數據、法律條文的時候，一定要自己查證。[RAG](rag.md) 和 [Prompt Engineering](prompt-engineering.md) 是目前常用來減少幻覺的方法。

---
**[← 回到 AI / 機器學習總覽](../README.md)**
