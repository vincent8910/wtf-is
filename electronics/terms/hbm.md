# HBM — 專門給 AI 晶片用的超快記憶體，把記憶體堆疊起來

> **白話說：** HBM 就像把好幾層 **停車場疊成立體停車塔**——同樣的佔地面積，能停的車（存的資料）多好幾倍，而且進出速度超快。AI 晶片靠它來餵飽龐大的運算需求。

---

## 它到底是什麼？

**HBM** 是 **High Bandwidth Memory（高頻寬記憶體）** 的縮寫。

它其實就是 [DRAM](dram.md) 的一種，但有兩個關鍵的不同：

1. **堆疊結構**：把好幾層 DRAM 晶片上下疊在一起，用「矽穿孔（TSV）」技術把每一層連通
2. **超高頻寬**：資料傳輸速度比普通 DRAM 快好幾倍

為什麼 AI 需要 HBM？

```
AI 訓練 / 推論 → 需要處理超大量資料
  ↓
GPU 運算速度很快，但一直在「等資料」
  ↓
普通 DRAM 餵資料太慢 → GPU 吃不飽 → 浪費算力
  ↓
HBM 頻寬超大 → 資料餵得夠快 → GPU 全力運轉
```

HBM 的發展：

| 版本 | 頻寬 | 說明 |
|------|------|------|
| HBM2 | 較低 | 早期版本 |
| HBM2E | 中等 | 主流版本 |
| HBM3 | 高 | NVIDIA H100 使用 |
| HBM3E | 更高 | NVIDIA H200 / B100 使用 |
| HBM4 | 最高 | 下一代，開發中 |

目前 HBM 主要由 **SK 海力士**（市佔第一）和**三星**生產，美光也在追趕。

## 生活比喻 / 實際例子

想像一個超級忙碌的餐廳廚房（[GPU](gpu.md)）：

- **普通 DRAM** = 一般的食材送貨（一次送一箱，廚師常常做完一道菜要等下一箱食材來）
- **HBM** = 直接在廚房旁邊蓋一個立體倉庫，用電梯連通（食材多、取用快，廚師不用等）
- 倉庫越多層（HBM 堆疊越多層）→ 存量越大 → 出餐越快

HBM 就是讓 AI 晶片「不再餓肚子」的關鍵零件。

實際你會聯到的說法：
- 新聞：「SK 海力士 **HBM3E** 獲得 NVIDIA 認證，訂單排到 2025 年」
- 法說會：「**HBM** 營收佔比已超過 DRAM 總營收的 30%」
- 分析師：「**HBM** 是 AI 半導體供應鏈中獲利最好的產品之一」

## 為什麼要知道這個詞？

- HBM 是 AI 半導體浪潮中最熱門的關鍵字之一
- SK 海力士因為 HBM 領先，股價和獲利都大幅成長
- 每一顆 NVIDIA AI 晶片都需要搭配多顆 HBM，用 [CoWoS](cowos.md) 封裝在一起
- HBM 的供需狀況直接影響 AI 晶片的出貨量和整個 AI 產業的發展速度

---
**[← 回到電子產業術語總覽](../README.md)**
